nohup: ignoring input
WARNING 12-19 10:13:31 config.py:2171] Casting torch.bfloat16 to torch.float16.
INFO 12-19 10:13:40 config.py:478] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 12-19 10:13:40 config.py:1216] Defaulting to use mp for distributed inference
WARNING 12-19 10:13:40 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 12-19 10:13:40 config.py:604] Async output processing is not supported on the current platform type cuda.
INFO 12-19 10:13:40 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
WARNING 12-19 10:13:41 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-19 10:13:41 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 12-19 10:13:42 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 12-19 10:13:42 selector.py:129] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:43 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:43 selector.py:129] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:43 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:43 selector.py:129] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:43 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:43 selector.py:129] Using XFormers backend.
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:44 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:44 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:44 multiproc_worker_utils.py:222] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:45 utils.py:922] Found nccl from library libnccl.so.2
INFO 12-19 10:13:45 utils.py:922] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:45 utils.py:922] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:45 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 12-19 10:13:45 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:45 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:45 utils.py:922] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:45 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=61622)[0;0m WARNING 12-19 10:13:46 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=61620)[0;0m WARNING 12-19 10:13:46 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=61621)[0;0m WARNING 12-19 10:13:46 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 12-19 10:13:46 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 12-19 10:13:46 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f0dbe69f'), local_subscribe_port=59479, remote_subscribe_port=None)
INFO 12-19 10:13:46 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:46 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:46 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:46 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...
INFO 12-19 10:13:47 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:47 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:47 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:47 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.50it/s]
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.28it/s]

INFO 12-19 10:13:51 model_runner.py:1097] Loading model weights took 3.3833 GB
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:13:52 model_runner.py:1097] Loading model weights took 3.3833 GB
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:13:53 model_runner.py:1097] Loading model weights took 3.3833 GB
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:13:53 model_runner.py:1097] Loading model weights took 3.3833 GB
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:14:18 worker.py:241] Memory profiling takes 25.19 seconds
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:14:18 worker.py:241] the current vLLM instance can use total_gpu_memory (15.57GiB) x gpu_memory_utilization (0.60) = 9.34GiB
[1;36m(VllmWorkerProcess pid=61621)[0;0m INFO 12-19 10:14:18 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 4.37GiB.
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:14:18 worker.py:241] Memory profiling takes 25.23 seconds
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:14:18 worker.py:241] the current vLLM instance can use total_gpu_memory (15.57GiB) x gpu_memory_utilization (0.60) = 9.34GiB
[1;36m(VllmWorkerProcess pid=61620)[0;0m INFO 12-19 10:14:18 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 4.37GiB.
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:14:18 worker.py:241] Memory profiling takes 25.23 seconds
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:14:18 worker.py:241] the current vLLM instance can use total_gpu_memory (15.57GiB) x gpu_memory_utilization (0.60) = 9.34GiB
[1;36m(VllmWorkerProcess pid=61622)[0;0m INFO 12-19 10:14:18 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 4.37GiB.
INFO 12-19 10:14:18 worker.py:241] Memory profiling takes 25.26 seconds
INFO 12-19 10:14:18 worker.py:241] the current vLLM instance can use total_gpu_memory (15.57GiB) x gpu_memory_utilization (0.60) = 9.34GiB
INFO 12-19 10:14:18 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 4.37GiB.
INFO 12-19 10:14:19 distributed_gpu_executor.py:57] # GPU blocks: 8944, # CPU blocks: 8192
INFO 12-19 10:14:19 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 4.37x
INFO 12-19 10:14:23 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 29.70 seconds
INFO:     Started server process [61013]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.17s/it, est. speed input: 229.85 toks/s, output: 29.36 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.18s/it, est. speed input: 229.85 toks/s, output: 29.36 toks/s]
INFO:     54.86.50.139:47348 - "POST /extract-travel%0A HTTP/1.1" 500 Internal Server Error
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.73s/it, est. speed input: 279.19 toks/s, output: 31.80 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.73s/it, est. speed input: 279.19 toks/s, output: 31.80 toks/s]
INFO:     54.86.50.139:59469 - "POST /extract-travel%0A HTTP/1.1" 500 Internal Server Error
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [61013]
/datadrive/pyenv/versions/3.10.12/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
